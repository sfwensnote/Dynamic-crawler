#!/usr/bin/env python3
"""
incremental_crawler.py - æ•™è‚²éƒ¨ç½‘ç«™å…¨é‡çˆ¬è™«ï¼ˆå¢é‡æ›´æ–°ç‰ˆï¼‰

åŠŸèƒ½ï¼š
  åŸºäº crawler.py çš„æ ¸å¿ƒé€»è¾‘ï¼Œå¢åŠ â€œè¿ç»­å·²å­˜åœ¨åˆ¤å®šâ€æœºåˆ¶ã€‚
  å½“è¿ç»­é‡åˆ° 20 ä¸ªå·²å­˜åœ¨çš„æ–‡ä»¶æ—¶ï¼Œè‡ªåŠ¨åœæ­¢å½“å‰æ¨¡å—çš„çˆ¬å–ã€‚
  
  é€‚ç”¨äºï¼šå®šæœŸè¿è¡Œï¼ˆå¦‚æ¯æ—¥/æ¯å‘¨ï¼‰ï¼ŒåªæŠ“å–æœ€æ–°å‘å¸ƒçš„æ–‡ä»¶ã€‚
"""

import os
import time
import logging
from urllib.parse import urljoin
from crawler import (
    SOURCES, BASE_DATA_DIR, load_existing_manifest,
    fetch_with_retry, get_total_pages_static, get_total_pages_dynamic,
    extract_items_from_static, extract_items_from_dynamic,
    download_detail, polite_sleep, logger
)

# è¿ç»­è·³è¿‡é˜ˆå€¼ï¼šå¦‚æœè¿ç»­è·³è¿‡ 20 ä¸ªæ–‡ä»¶ï¼Œè®¤ä¸ºåç»­éƒ½æ˜¯æ—§æ–‡ä»¶ï¼Œåœæ­¢çˆ¬å–
CONSECUTIVE_SKIP_LIMIT = 20

def crawl_static_incremental(source: dict, existing_urls: set, max_pages: int = None):
    """å¢é‡çˆ¬å–é™æ€åˆ†é¡µæ ç›®"""
    name = source["name"]
    base_url = source["base_url"]
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¢é‡æ›´æ–°: {name}")
    logger.info(f"{'='*60}")

    # ç¬¬1é¡µ
    first_url = urljoin(base_url, "index.html")
    resp = fetch_with_retry(first_url)
    if not resp:
        logger.error(f"æ— æ³•è®¿é—® {name} é¦–é¡µ")
        return

    total_pages = get_total_pages_static(resp.text)
    if max_pages:
        total_pages = min(total_pages, max_pages)
    
    # å¢é‡æ¨¡å¼é€šå¸¸ä¸éœ€è¦çˆ¬å¾ˆå¤šé¡µï¼Œä½†æˆ‘ä»¬ä»ä¿ç•™ç¿»é¡µé€»è¾‘ï¼Œé  skip æœºåˆ¶é€€å‡º
    logger.info(f"{name}:æ£€æµ‹åˆ°å…± {total_pages} é¡µï¼Œå°†æ‰§è¡Œå¢é‡æ£€æŸ¥...")

    stats = {"downloaded": 0, "skipped": 0, "failed": 0}
    consecutive_skips = 0
    stop_signal = False

    for page_num in range(1, total_pages + 1):
        if stop_signal:
            break

        if page_num == 1:
            page_url = urljoin(base_url, "index.html")
            html = resp.text
        else:
            page_url = urljoin(base_url, f"index_{page_num - 1}.html")
            polite_sleep(1, 3)
            page_resp = fetch_with_retry(page_url, retries=1)
            if not page_resp:
                logger.warning(f"{name}: ç¬¬ {page_num} é¡µè·å–å¤±è´¥ (404?)ï¼Œå·²åˆ°è¾¾æœ«å°¾")
                break
            html = page_resp.text

        items = extract_items_from_static(html, page_url)
        logger.info(f"{name}: ç¬¬ {page_num} é¡µ, è§£æåˆ° {len(items)} æ¡")

        if not items:
            logger.warning(f"{name}: ç¬¬ {page_num} é¡µæ— æœ‰æ•ˆæ¡ç›®ï¼Œåœæ­¢")
            break

        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                consecutive_skips = 0  # é‡ç½®è®¡æ•°å™¨
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1
                consecutive_skips += 1
            
            if consecutive_skips >= CONSECUTIVE_SKIP_LIMIT:
                logger.info(f"âš¡ï¸ è¿ç»­è·³è¿‡ {consecutive_skips} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼Œåˆ¤å®šä¸ºæ— æ–°å†…å®¹ã€‚")
                logger.info(f"ğŸ›‘ åœæ­¢çˆ¬å–æ¨¡å—: {name}")
                stop_signal = True
                break

    logger.info(f"{name} å¢é‡æ›´æ–°å®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")


def crawl_dynamic_incremental(source: dict, existing_urls: set, max_pages: int = None):
    """å¢é‡çˆ¬å–åŠ¨æ€åˆ†é¡µæ ç›®"""
    name = source["name"]
    base_url = source["base_url"]
    params_template = source.get("params", {})
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¢é‡æ›´æ–°: {name}")
    logger.info(f"{'='*60}")

    # ç¬¬1é¡µ
    params = params_template.copy()
    resp = fetch_with_retry(base_url, params=params)
    if not resp:
        logger.error(f"æ— æ³•è®¿é—® {name} é¦–é¡µ")
        return

    total_pages = get_total_pages_dynamic(resp.text)
    if max_pages:
        total_pages = min(total_pages, max_pages)

    logger.info(f"{name}:æ£€æµ‹åˆ°å…± {total_pages} é¡µï¼Œå°†æ‰§è¡Œå¢é‡æ£€æŸ¥...")

    stats = {"downloaded": 0, "skipped": 0, "failed": 0}
    consecutive_skips = 0
    stop_signal = False

    # åŠ¨æ€é¡µç¿»é¡µï¼šé€šå¸¸åªéœ€å‰å‡ é¡µ
    for page_num in range(1, total_pages + 1):
        if stop_signal:
            break

        if page_num == 1:
            html = resp.text
            page_url = base_url
        else:
            params = params_template.copy()
            params["page"] = page_num
            polite_sleep(1, 3)
            page_resp = fetch_with_retry(base_url, params=params)
            if not page_resp:
                logger.warning(f"Failed to fetch page {page_num}")
                stats["failed"] += 1
                continue
            html = page_resp.text
            page_url = f"{base_url}?page={page_num}"

        items = extract_items_from_dynamic(html, base_url)
        logger.info(f"{name}: ç¬¬ {page_num} é¡µ, è§£æåˆ° {len(items)} æ¡")

        if not items:
            logger.warning(f"{name}: ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œåœæ­¢")
            break

        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                consecutive_skips = 0
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1
                consecutive_skips += 1
            
            if consecutive_skips >= CONSECUTIVE_SKIP_LIMIT:
                logger.info(f"âš¡ï¸ è¿ç»­è·³è¿‡ {consecutive_skips} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼Œåˆ¤å®šä¸ºæ— æ–°å†…å®¹ã€‚")
                logger.info(f"ğŸ›‘ åœæ­¢çˆ¬å–æ¨¡å—: {name}")
                stop_signal = True
                break

    logger.info(f"{name} å¢é‡æ›´æ–°å®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")



def main():
    os.makedirs(BASE_DATA_DIR, exist_ok=True)
    existing_urls = load_existing_manifest()
    logger.info(f"å·²åŠ è½½ {len(existing_urls)} æ¡å†å²è®°å½•")

    for source in SOURCES:
        try:
            if source["type"] == "static":
                crawl_static_incremental(source, existing_urls)
            elif source["type"] == "dynamic":
                crawl_dynamic_incremental(source, existing_urls)
        except Exception as e:
            logger.error(f"Source {source['name']} failed: {e}")

if __name__ == "__main__":
    main()
