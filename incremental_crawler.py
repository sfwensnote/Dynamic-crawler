#!/usr/bin/env python3
"""
incremental_crawler.py - æ•™è‚²éƒ¨ç½‘ç«™çº¯å¢é‡çˆ¬è™«ï¼ˆæœ€ä¿é™©ç‰ˆï¼‰

ç®—æ³•ï¼š
  é™æ€æ ç›®ï¼ˆä¸­å¤®æ–‡ä»¶ / å…¶ä»–éƒ¨é—¨æ–‡ä»¶ï¼‰ï¼š
    â†’ é¡µæ•°å¾ˆå°‘ï¼ˆ10-20é¡µï¼‰ï¼Œæ¯æ¬¡è¿è¡Œå…¨é¡µæ‰«æï¼Œé›¶é—æ¼ã€‚
    
  åŠ¨æ€æ ç›®ï¼ˆæ•™è‚²éƒ¨æ–‡ä»¶ï¼‰ï¼š
    â†’ é¡µæ•°å¤šï¼ˆ800+é¡µï¼‰ï¼Œé‡‡ç”¨"è¿ç»­æ•´é¡µè·³è¿‡"ç­–ç•¥ï¼š
      å½“è¿ç»­ 3 æ•´é¡µçš„æ‰€æœ‰æ¡ç›®éƒ½å·²å­˜åœ¨æ—¶ï¼Œæ‰åœæ­¢ã€‚
      æ¯”"è¿ç»­Næ¡è·³è¿‡"æ›´ä¿é™©ï¼Œå› ä¸ºé¢—ç²’åº¦æ˜¯æ•´é¡µè€Œéå•æ¡ã€‚
"""

import os
import itertools
from urllib.parse import urljoin
from crawler import (
    SOURCES, BASE_DATA_DIR, load_existing_manifest,
    fetch_with_retry, extract_items_from_static, extract_items_from_dynamic,
    download_detail, polite_sleep, logger
)

# åŠ¨æ€æ ç›®ï¼šè¿ç»­å¤šå°‘æ•´é¡µå…¨éƒ¨ä¸ºæ—§æ–‡ä»¶æ—¶æ‰åœæ­¢
FULL_PAGE_SKIP_LIMIT = 3


def crawl_static_full_scan(source: dict, existing_urls: set):
    """
    é™æ€æ ç›®å…¨é¡µæ‰«æã€‚
    é¡µæ•°å¾ˆå°‘ï¼ˆ10-20é¡µï¼‰ï¼Œæ¯æ¬¡éƒ½æ‰«å®Œï¼Œç»å¯¹ä¸é—æ¼ã€‚
    """
    name = source["name"]
    base_url = source["base_url"]
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å…¨é¡µæ‰«æ: {name}")
    logger.info(f"{'='*60}")

    stats = {"downloaded": 0, "skipped": 0}

    for page_num in itertools.count(1):
        if page_num == 1:
            page_url = urljoin(base_url, "index.html")
        else:
            page_url = urljoin(base_url, f"index_{page_num - 1}.html")
            polite_sleep(1, 2)

        resp = fetch_with_retry(page_url, retries=1)
        if not resp:
            logger.info(f"{name}: ç¬¬ {page_num} é¡µä¸å­˜åœ¨ï¼Œæ‰«æç»“æŸ")
            break

        items = extract_items_from_static(resp.text, page_url)
        if not items:
            logger.info(f"{name}: ç¬¬ {page_num} é¡µæ— æœ‰æ•ˆå†…å®¹ï¼Œæ‰«æç»“æŸ")
            break

        page_new = 0
        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                page_new += 1
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1

        logger.info(f"{name}: ç¬¬ {page_num} é¡µå®Œæˆ (æ–°å¢ {page_new}, è·³è¿‡ {len(items) - page_new})")

    logger.info(f"âœ… {name} æ‰«æå®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")


def crawl_dynamic_incremental(source: dict, existing_urls: set):
    """
    åŠ¨æ€æ ç›®å¢é‡æ‰«æã€‚
    é‡‡ç”¨"è¿ç»­æ•´é¡µè·³è¿‡"ç­–ç•¥ï¼šåªæœ‰è¿ç»­ 3 æ•´é¡µå…¨éƒ¨ä¸ºæ—§æ–‡ä»¶æ—¶æ‰åœæ­¢ã€‚
    æ¯”"è¿ç»­Næ¡"æ›´ä¿é™©ï¼Œå³ä½¿æ–‡ä»¶æ•£è½åœ¨ä¸åŒä½ç½®ä¹Ÿèƒ½æ•è·ã€‚
    """
    name = source["name"]
    base_url = source["base_url"]
    params_template = source.get("params", {})
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¢é‡æ‰«æ: {name}")
    logger.info(f"{'='*60}")

    stats = {"downloaded": 0, "skipped": 0}
    consecutive_full_skip_pages = 0  # è¿ç»­æ•´é¡µå…¨æ—§çš„é¡µæ•°

    for page_num in itertools.count(1):
        params = params_template.copy()
        if page_num > 1:
            params["page"] = page_num
            polite_sleep(1, 3)

        resp = fetch_with_retry(base_url, params=params)
        if not resp:
            logger.warning(f"{name}: ç¬¬ {page_num} é¡µè·å–å¤±è´¥")
            break

        items = extract_items_from_dynamic(resp.text, base_url)
        if not items:
            logger.info(f"{name}: ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œæ‰«æç»“æŸ")
            break

        # é€æ¡æ£€æŸ¥
        page_new = 0
        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                page_new += 1
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1

        logger.info(f"{name}: ç¬¬ {page_num} é¡µå®Œæˆ (æ–°å¢ {page_new}, è·³è¿‡ {len(items) - page_new})")

        # åˆ¤æ–­æ•´é¡µæ˜¯å¦å…¨éƒ¨ä¸ºæ—§æ–‡ä»¶
        if page_new == 0:
            consecutive_full_skip_pages += 1
            if consecutive_full_skip_pages >= FULL_PAGE_SKIP_LIMIT:
                logger.info(f"âš¡ï¸ è¿ç»­ {FULL_PAGE_SKIP_LIMIT} æ•´é¡µå‡ä¸ºå·²å­˜åœ¨æ–‡ä»¶ï¼Œå·²è¿½å¹³å†å²è¿›åº¦ã€‚")
                logger.info(f"ğŸ›‘ åœæ­¢æ‰«æ: {name}")
                break
        else:
            consecutive_full_skip_pages = 0  # æœ‰æ–°æ–‡ä»¶ï¼Œé‡ç½®è®¡æ•°

    logger.info(f"âœ… {name} æ‰«æå®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")


def main():
    os.makedirs(BASE_DATA_DIR, exist_ok=True)
    existing_urls = load_existing_manifest()
    logger.info(f"å·²åŠ è½½ {len(existing_urls)} æ¡å†å²è®°å½•")

    total_new = 0
    for source in SOURCES:
        try:
            if source["type"] == "static":
                crawl_static_full_scan(source, existing_urls)
            elif source["type"] == "dynamic":
                crawl_dynamic_incremental(source, existing_urls)
        except Exception as e:
            logger.error(f"æ¨¡å— {source['name']} å‡ºé”™: {e}")

    logger.info("å…¨éƒ¨æ¨¡å—æ‰«æå®Œæˆã€‚")


if __name__ == "__main__":
    main()
