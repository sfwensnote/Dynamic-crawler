#!/usr/bin/env python3
"""
incremental_crawler.py - æ•™è‚²éƒ¨ç½‘ç«™çº¯å¢é‡çˆ¬è™«

åŠŸèƒ½ï¼š
  ä¸“ä¸ºâ€œå®šæœŸæ›´æ–°â€è®¾è®¡ï¼Œå‰”é™¤äº†å…¨é‡çˆ¬å–çš„å¤æ‚é€»è¾‘ã€‚
  åŸç†ï¼šåªçˆ¬å–åˆ—è¡¨é¡µçš„å‰å‡ é¡µï¼Œä¸€æ—¦è¿ç»­é‡åˆ° 20 ä¸ªå·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œç«‹å³åœæ­¢ã€‚
  
  ç‰¹ç‚¹ï¼š
  - æé€Ÿï¼šä¸åšå…¨é‡ç¿»é¡µæ£€æµ‹ï¼Œé€šå¸¸ä»…è¯·æ±‚ 1-3 é¡µå³å¯å®Œæˆã€‚
  - çœæµï¼šä»…ä¸‹è½½æ–°æ–‡ä»¶ã€‚
"""

import os
import itertools
from urllib.parse import urljoin
from crawler import (
    SOURCES, BASE_DATA_DIR, load_existing_manifest,
    fetch_with_retry, extract_items_from_static, extract_items_from_dynamic,
    download_detail, polite_sleep, logger
)

# è¿ç»­è·³è¿‡é˜ˆå€¼
CONSECUTIVE_SKIP_LIMIT = 20

def crawl_static_incremental(source: dict, existing_urls: set):
    """å¢é‡çˆ¬å–é™æ€åˆ†é¡µæ ç›®"""
    name = source["name"]
    base_url = source["base_url"]
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¢é‡æ‰«æ: {name}")
    logger.info(f"{'='*60}")

    stats = {"downloaded": 0, "skipped": 0, "failed": 0}
    consecutive_skips = 0

    # æ— é™å¾ªç¯ç¿»é¡µï¼Œç›´åˆ°è§¦å‘åœæ­¢æ¡ä»¶
    for page_num in itertools.count(1):
        if page_num == 1:
            page_url = urljoin(base_url, "index.html")
            resp = fetch_with_retry(page_url)
        else:
            page_url = urljoin(base_url, f"index_{page_num - 1}.html")
            polite_sleep(1, 3)
            resp = fetch_with_retry(page_url, retries=1)

        if not resp:
            logger.info(f"{name}: ç¬¬ {page_num} é¡µæ— æ³•è·å– (å¯èƒ½æ˜¯ç¿»å®Œæˆ–404)ï¼Œåœæ­¢æ‰«æ")
            break
        
        items = extract_items_from_static(resp.text, page_url)
        logger.info(f"{name}: ç¬¬ {page_num} é¡µ, è§£æåˆ° {len(items)} æ¡")

        if not items:
            logger.warning(f"{name}: ç¬¬ {page_num} é¡µæ— æœ‰æ•ˆå†…å®¹ï¼Œåœæ­¢")
            break

        # æ£€æŸ¥æœ¬é¡µå†…å®¹
        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                consecutive_skips = 0
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1
                consecutive_skips += 1
            
            if consecutive_skips >= CONSECUTIVE_SKIP_LIMIT:
                logger.info(f"âš¡ï¸ è¿ç»­è·³è¿‡ {consecutive_skips} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼Œå·²è¿½å¹³å†å²è¿›åº¦ã€‚")
                logger.info(f"ğŸ›‘ åœæ­¢çˆ¬å–æ¨¡å—: {name}")
                logger.info(f"{name} å¢é‡æ‰«æå®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")
                return

    logger.info(f"{name} æ‰«æç»“æŸ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")


def crawl_dynamic_incremental(source: dict, existing_urls: set):
    """å¢é‡çˆ¬å–åŠ¨æ€åˆ†é¡µæ ç›®"""
    name = source["name"]
    base_url = source["base_url"]
    params_template = source.get("params", {})
    save_dir = os.path.join(BASE_DATA_DIR, source["dir_name"])
    os.makedirs(save_dir, exist_ok=True)

    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹å¢é‡æ‰«æ: {name}")
    logger.info(f"{'='*60}")

    stats = {"downloaded": 0, "skipped": 0, "failed": 0}
    consecutive_skips = 0

    for page_num in itertools.count(1):
        params = params_template.copy()
        if page_num > 1:
            params["page"] = page_num
        
        if page_num > 1:
            polite_sleep(1, 3)
            
        resp = fetch_with_retry(base_url, params=params)
        if not resp:
            logger.warning(f"Failed to fetch page {page_num}")
            stats["failed"] += 1
            if stats["failed"] > 3: # è¿ç»­å¤±è´¥å‡ æ¬¡å°±åœå§
                break
            continue

        items = extract_items_from_dynamic(resp.text, base_url)
        logger.info(f"{name}: ç¬¬ {page_num} é¡µ, è§£æåˆ° {len(items)} æ¡")

        if not items:
            logger.info(f"{name}: ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œåœæ­¢")
            break

        for item in items:
            is_new = download_detail(item, save_dir, existing_urls, name)
            if is_new:
                stats["downloaded"] += 1
                consecutive_skips = 0
                logger.info(f"  âœ“ æ–°å¢: {item['date']} {item['title'][:40]}...")
            else:
                stats["skipped"] += 1
                consecutive_skips += 1
            
            if consecutive_skips >= CONSECUTIVE_SKIP_LIMIT:
                logger.info(f"âš¡ï¸ è¿ç»­è·³è¿‡ {consecutive_skips} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼Œå·²è¿½å¹³å†å²è¿›åº¦ã€‚")
                logger.info(f"ğŸ›‘ åœæ­¢çˆ¬å–æ¨¡å—: {name}")
                logger.info(f"{name} å¢é‡æ‰«æå®Œæˆ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")
                return

    logger.info(f"{name} æ‰«æç»“æŸ: æ–°å¢ {stats['downloaded']}, è·³è¿‡ {stats['skipped']}")


def main():
    os.makedirs(BASE_DATA_DIR, exist_ok=True)
    existing_urls = load_existing_manifest()
    logger.info(f"å·²åŠ è½½ {len(existing_urls)} æ¡å†å²è®°å½•")

    for source in SOURCES:
        try:
            if source["type"] == "static":
                crawl_static_incremental(source, existing_urls)
            elif source["type"] == "dynamic":
                crawl_dynamic_incremental(source, existing_urls)
        except Exception as e:
            logger.error(f"Source {source['name']} failed: {e}")

if __name__ == "__main__":
    main()
