# 增量爬取与数据更新设计文档

**目标**: 实现定期检查各分类是否有更新，仅下载新增文件，保留原始HTML，避免全量重复爬取。

## 1. 核心设计原则

1.  **基于 Manifest 的增量检测**: 利用已有的 `data/manifest.jsonl` 作为数据库。每次运行时，先加载所有已爬取的 URL。
2.  **“连续跳过”中断机制 (Consecutive Skip Break)**:
    *   在爬取列表页时，如果连续遇到 N 个（例如 20 个）已经存在于 Manifest 中的 URL，则认为后续数据均为旧数据，**立即停止**该模块的爬取。
    *   此机制利用了网站列表通常按时间倒序排列的特性。
3.  **原始 HTML 归档**:
    *   保持现有的 `save_raw_html` 逻辑不变。每个 URL 对应一个唯一的 hash 文件名。
    *   只有当 URL 不在 `manifest.jsonl` 中时，才请求详情页并保存 HTML。

## 2. 模块策略

### 2.1 静态分页模块 (中央文件 / 其他部门文件)
*   **由于页数很少 (10-20页)**:
    *   策略 A: 依旧全量遍历所有列表页（耗时极短，几十秒）。
    *   策略 B: 同样应用“连续跳过”机制。如果第 1 页的所有条目都已存在，则不继续爬取第 2 页。
*   **推荐**: 策略 B。效率最高。

### 2.2 动态搜索模块 (教育部文件)
*   **由于页数很多 (800+页)**:
    *   **严格应用“连续跳过”机制**。
    *   一旦检测到连续 50 个条目（跨越 2.5 页）都是已下载的，则终止翻页。
    *   这样每次更新只需爬取前 3-5 页即可完成，耗时仅需几秒钟。

## 3. 实现方案 (独立脚本)

创建独立脚本 `incremental_crawler.py`，去除全量爬取的所有复杂逻辑（如计算总页数、断点续爬等），专注于“扫描-发现-下载”。

*   **纯增量模式**:
    *   采用 `itertools.count(1)` 无限循环翻页。
    *   每页解析后，逐条检查 URL 是否在 `manifest.jsonl` 中。
    *   维护 `consecutive_skips` 计数器。
    *   **触发阈值 (20)**：一旦连续 20 个条目均已存在，立即终止该模块的扫描。
    *   不再需要指定 `max_pages`，完全由数据状态决定何时停止。

这种设计使得增量更新变得极轻量级，通常只需请求 1-3 页即可完成所有模块的检查。

## 4. 自动化运行 (Cron / CI)

建议配置每天或每周运行一次：
```bash
# 每天凌晨 2 点运行增量更新
0 2 * * * cd /path/to/crawler && source .venv/bin/activate && python crawler.py --incremental >> logs/cron.log 2>&1
```

## 5. 数据结构保持

无需修改现有的文件存储结构：
- `data/manifest.jsonl`: 继续追加新记录。
- `data/raw_html/`: 继续存入新哈希文件。
- `data/module/`: 继续存入重命名后的新文件。

这种设计既满足了“保留原始HTML”的需求，又实现了极低成本的“定期Check”。
