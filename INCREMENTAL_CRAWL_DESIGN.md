# 增量爬取与数据更新设计文档

**目标**: 实现定期检查各分类是否有更新，仅下载新增文件，保留原始HTML，避免全量重复爬取。

## 1. 核心设计原则

1.  **基于 Manifest 的增量检测**: 利用已有的 `data/manifest.jsonl` 作为数据库。每次运行时，先加载所有已爬取的 URL。
2.  **“连续跳过”中断机制 (Consecutive Skip Break)**:
    *   在爬取列表页时，如果连续遇到 N 个（例如 20 个）已经存在于 Manifest 中的 URL，则认为后续数据均为旧数据，**立即停止**该模块的爬取。
    *   此机制利用了网站列表通常按时间倒序排列的特性。
3.  **原始 HTML 归档**:
    *   保持现有的 `save_raw_html` 逻辑不变。每个 URL 对应一个唯一的 hash 文件名。
    *   只有当 URL 不在 `manifest.jsonl` 中时，才请求详情页并保存 HTML。

## 2. 模块策略

### 2.1 静态分页模块 (中央文件 / 其他部门文件)
*   **由于页数很少 (10-20页)**:
    *   策略 A: 依旧全量遍历所有列表页（耗时极短，几十秒）。
    *   策略 B: 同样应用“连续跳过”机制。如果第 1 页的所有条目都已存在，则不继续爬取第 2 页。
*   **推荐**: 策略 B。效率最高。

### 2.2 动态搜索模块 (教育部文件)
*   **由于页数很多 (800+页)**:
    *   **严格应用“连续跳过”机制**。
    *   一旦检测到连续 50 个条目（跨越 2.5 页）都是已下载的，则终止翻页。
    *   这样每次更新只需爬取前 3-5 页即可完成，耗时仅需几秒钟。

## 3. 实现方案 (CLI 参数)

增加 `--incremental` (或 `--update`) 参数。

*   **全量模式 (Default)**:
    *   爬取所有页码 (`range(1, total_pages)`)。
    *   即使遇到重复项也继续翻页（主要用于补漏或验证）。
*   **增量模式 (`--incremental`)**:
    *   启用 `consecutive_skip_count` 计数器。
    *   每遇到一个新文件，重置计数器。
    *   每遇到一个旧文件，计数器 +1。
    *   当 `consecutive_skip_count >= THRESHOLD` (如 20)，抛出 `StopIteration` 或 `break`。

## 4. 自动化运行 (Cron / CI)

建议配置每天或每周运行一次：
```bash
# 每天凌晨 2 点运行增量更新
0 2 * * * cd /path/to/crawler && source .venv/bin/activate && python crawler.py --incremental >> logs/cron.log 2>&1
```

## 5. 数据结构保持

无需修改现有的文件存储结构：
- `data/manifest.jsonl`: 继续追加新记录。
- `data/raw_html/`: 继续存入新哈希文件。
- `data/module/`: 继续存入重命名后的新文件。

这种设计既满足了“保留原始HTML”的需求，又实现了极低成本的“定期Check”。
